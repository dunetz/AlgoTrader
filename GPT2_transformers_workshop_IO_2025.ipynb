{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "sourceId": 10577797,
          "sourceType": "datasetVersion",
          "datasetId": 5848741
        }
      ],
      "dockerImageVersionId": 30920,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dunetz/AlgoTrader/blob/master/GPT2_transformers_workshop_IO_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrain the GPT2 model\n",
        "\n",
        "This notebook seeks to replicate Andrey Karpathy's highly popular [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master) project and trains a GPT2 model from scratch using JAX+TPU and the [OpenWebText dataset](https://huggingface.co/datasets/Skylion007/openwebtext).\n",
        "\n",
        "You can run this on Colab, Kaggle or GCP TPUs, although Colab TPU v2 can only run the smallest 124M GPT model.\n"
      ],
      "metadata": {
        "id": "rvP1eNN_pExM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determine platform"
      ],
      "metadata": {
        "id": "LD3bo9FxhrTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume using Cloud TPU by default\n",
        "platform = \"GCP\"\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  platform = \"Colab\"\n",
        "except ImportError:\n",
        "  pass # not colab\n",
        "\n",
        "try:\n",
        "  import kaggle\n",
        "  platform = \"Kaggle\"\n",
        "except (ImportError, IOError):\n",
        "  pass # not kaggle\n",
        "\n",
        "print(f\"Platform is {platform}\")"
      ],
      "metadata": {
        "id": "MuFEBtNkuAdT",
        "outputId": "6ceb5a55-9a7a-4efc-f00a-2778d5e3562d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Platform is Colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ],
      "metadata": {
        "id": "hTmz5Cbco7n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jax-ai-stack\n",
        "if platform == \"Colab\": # temp workaround on Colab (https://github.com/jax-ml/jax-ai-stack/issues/149)\n",
        "  !pip install -Uq \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -Uq tiktoken matplotlib kaggle wandb tpu-info\n"
      ],
      "metadata": {
        "id": "6zMsOIc7ouCO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:30:18.414058Z",
          "iopub.execute_input": "2025-02-21T04:30:18.414249Z",
          "iopub.status.idle": "2025-02-21T04:30:51.718249Z",
          "shell.execute_reply.started": "2025-02-21T04:30:18.414229Z",
          "shell.execute_reply": "2025-02-21T04:30:51.716902Z"
        },
        "outputId": "ddca12cb-623a-4ca6-d3b4-54f88e5ef719",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/99.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/456.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m450.6/456.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.0/456.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.2/319.2 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.3/406.3 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.5/136.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax-ai-stack 2025.4.9 requires jax==0.5.3, but you have jax 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.7/356.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm we have TPUs set up."
      ],
      "metadata": {
        "id": "6cWxBvz6bZDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "uZUaKdi5bSEN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:30:51.719166Z",
          "iopub.execute_input": "2025-02-21T04:30:51.719450Z",
          "iopub.status.idle": "2025-02-21T04:30:59.572796Z",
          "shell.execute_reply.started": "2025-02-21T04:30:51.719422Z",
          "shell.execute_reply": "2025-02-21T04:30:59.571540Z"
        },
        "outputId": "ff785ba8-bf8b-40a9-a34d-a8679c84c642",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take care of the imports."
      ],
      "metadata": {
        "id": "sKE2uUafLobI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax, orbax\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import numpy as np\n",
        "import tiktoken, time, wandb"
      ],
      "metadata": {
        "id": "MKYFNOhdLq98",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:30:59.573640Z",
          "iopub.execute_input": "2025-02-21T04:30:59.573940Z",
          "iopub.status.idle": "2025-02-21T04:31:01.600418Z",
          "shell.execute_reply.started": "2025-02-21T04:30:59.573916Z",
          "shell.execute_reply": "2025-02-21T04:31:01.598633Z"
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "Define the device mesh.\n"
      ],
      "metadata": {
        "id": "rPyt7MV6prz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Alternative data and model parallel\n",
        "# mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
        "\n",
        "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))"
      ],
      "metadata": {
        "id": "xuMlCK3Q8WJD",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:31:01.601360Z",
          "iopub.execute_input": "2025-02-21T04:31:01.601598Z",
          "iopub.status.idle": "2025-02-21T04:31:01.605772Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.601574Z",
          "shell.execute_reply": "2025-02-21T04:31:01.604615Z"
        }
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the GPT-2 tokenizer via OpenAI's [Tiktoken](https://github.com/openai/tiktoken) library."
      ],
      "metadata": {
        "id": "_ZKdhNo98NgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "iWbkk1V7-Isg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:31:01.606708Z",
          "iopub.execute_input": "2025-02-21T04:31:01.606937Z",
          "iopub.status.idle": "2025-02-21T04:31:04.402839Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.606915Z",
          "shell.execute_reply": "2025-02-21T04:31:04.401628Z"
        }
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set some hyperparameters."
      ],
      "metadata": {
        "id": "igX_eoGNMTGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "GPT2_variant = \"GPT2\" # \"GPT2-medium\"\n",
        "if GPT2_variant == \"GPT2-medium\":\n",
        "  num_transformer_blocks = 24\n",
        "  seqlen = 1024\n",
        "  embed_dim = 1024\n",
        "  num_heads = 16\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  batch_size = 32  # Can only run on TPU v3+\n",
        "else: ## Assume GPT2 otherwise\n",
        "  num_transformer_blocks = 12\n",
        "  seqlen = 1024\n",
        "  embed_dim = 768\n",
        "  num_heads = 12\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  if platform == \"Colab\":\n",
        "      batch_size = 24 # TPU v2\n",
        "  else:\n",
        "      batch_size = 72 # TPU v3\n",
        "\n",
        "dropout_rate = 0.1\n",
        "\n",
        "max_steps = 600000*12//batch_size\n",
        "# Kaggle TPU limit per session is 9 hours, which is ~95K steps for GPT2\n",
        "if platform == \"Kaggle\":\n",
        "  max_steps = 90000\n",
        "init_learning_rate = 5e-4\n",
        "weight_decay = 1e-1\n",
        "top_k = 10\n",
        "dtype = jnp.bfloat16\n",
        "param_dtype = jnp.float32"
      ],
      "metadata": {
        "id": "GRhiDsCrMZRp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:00.706531Z",
          "iopub.execute_input": "2025-02-21T04:32:00.706850Z",
          "iopub.status.idle": "2025-02-21T04:32:00.712524Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.706823Z",
          "shell.execute_reply": "2025-02-21T04:32:00.711567Z"
        }
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now define the model architecture. You can refer to [OpenAI's official implementation](https://github.com/openai/gpt-2) or [nanoGPT](https://github.com/karpathy/nanoGPT) for comparison. Main difference is with the sharding scheme."
      ],
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float, rngs: nnx.Rngs):\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                          dtype=dtype,\n",
        "                                          param_dtype=param_dtype,\n",
        "                                          rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=dropout_rate)  # Added dropout layer after MHA\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        bs, seq_len, emb_sz = input_shape\n",
        "\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=self.layer_norm1(inputs),\n",
        "            mask=causal_attention_mask(seq_len),\n",
        "            decode=False,\n",
        "        )\n",
        "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.linear1(self.layer_norm2(x))\n",
        "        mlp_output = nnx.gelu(mlp_output)\n",
        "        mlp_output = self.linear2(mlp_output)\n",
        "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
        "\n",
        "        return x + mlp_output\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=seqlen, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return self.token_emb, token_embedding+position_embedding\n",
        "\n",
        "\n",
        "class GPT2(nnx.Module):\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, num_heads: int, rate: float, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    seqlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.dropout = nnx.Dropout(rate=rate)\n",
        "\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, dropout_rate, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)]\n",
        "\n",
        "        self.layer_norm = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                    num_features=embed_dim,\n",
        "                                    scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                    bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                    dtype=dtype,\n",
        "                                    param_dtype=param_dtype,\n",
        "                                    rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        token_embedding, x = self.embedding_layer(inputs)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        x = self.layer_norm(x)\n",
        "        # Weights tying\n",
        "        outputs = token_embedding.attend(x)\n",
        "        return outputs\n",
        "\n",
        "    @nnx.jit\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    @nnx.jit\n",
        "    def generate_step(self, padded_tokens, sample_index):\n",
        "        logits = self(padded_tokens)\n",
        "        next_token = self.sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    def generate_text(self, max_tokens, start_tokens):\n",
        "        generated = []\n",
        "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
        "        for i in range(max_tokens):\n",
        "            sample_index = len(start_tokens) + len(generated) - 1\n",
        "            # TODO: use attention masking for better efficiency\n",
        "            padded_tokens = jnp.array((start_tokens + generated + [0] * (seqlen - len(start_tokens) - len(generated))))[None, :]\n",
        "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
        "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
        "              break\n",
        "            generated.append(next_token)\n",
        "            # decode and print next_token\n",
        "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "def create_model(rngs):\n",
        "    return GPT2(seqlen, vocab_size, embed_dim, num_heads, dropout_rate, feed_forward_dim, num_transformer_blocks, rngs=rngs)"
      ],
      "metadata": {
        "id": "z0p-IHurrB9i",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:00.771150Z",
          "iopub.execute_input": "2025-02-21T04:32:00.771416Z",
          "iopub.status.idle": "2025-02-21T04:32:00.792958Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.771393Z",
          "shell.execute_reply": "2025-02-21T04:32:00.791974Z"
        }
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Weights and Biases to track training progress."
      ],
      "metadata": {
        "id": "eBfT1dp5hMUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "VwtNNDa4jVk7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import userdata\n",
        "  os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "elif platform == \"Kaggle\":\n",
        "  from kaggle_secrets import UserSecretsClient\n",
        "  user_secrets = UserSecretsClient()\n",
        "  os.environ['WANDB_API_KEY'] = user_secrets.get_secret('WANDB_API_KEY')\n",
        "else:\n",
        "  print(\"Please set the WANDB_API_KEY env variable manually\") #input()\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project='GPT2-pretraining',\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "      'architecture': GPT2_variant,\n",
        "      'dataset': 'OpenWebText',\n",
        "      'platform': platform,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'dtype': dtype,\n",
        "      'param_dtype': param_dtype,\n",
        "      'init_learning_rate': init_learning_rate,\n",
        "      'num_transformer_blocks': num_transformer_blocks,\n",
        "      'seqlen': seqlen,\n",
        "      'embed_dim': embed_dim,\n",
        "      'num_heads': num_heads,\n",
        "      'feed_forward_dim': feed_forward_dim,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'weight_decay': weight_decay\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "IbhEtsganEWg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:25.482072Z",
          "iopub.execute_input": "2025-02-21T04:32:25.482477Z",
          "iopub.status.idle": "2025-02-21T04:32:28.629414Z",
          "shell.execute_reply.started": "2025-02-21T04:32:25.482449Z",
          "shell.execute_reply": "2025-02-21T04:32:28.628576Z"
        },
        "outputId": "941e3cb7-aedf-4e45-ec83-56b5b756c71e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malan-dunetz\u001b[0m (\u001b[33malan-dunetz-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250727_210711-htqux531</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alan-dunetz-none/GPT2-pretraining/runs/htqux531' target=\"_blank\">morning-dew-2</a></strong> to <a href='https://wandb.ai/alan-dunetz-none/GPT2-pretraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alan-dunetz-none/GPT2-pretraining' target=\"_blank\">https://wandb.ai/alan-dunetz-none/GPT2-pretraining</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alan-dunetz-none/GPT2-pretraining/runs/htqux531' target=\"_blank\">https://wandb.ai/alan-dunetz-none/GPT2-pretraining/runs/htqux531</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alan-dunetz-none/GPT2-pretraining/runs/htqux531?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7e6b046f20d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data\n",
        "\n",
        "The goal is to have `train.bin` and `val.bin` in the same `data_dir`.\n",
        "\n",
        "* For Colab: place the files in Google Drive, at `/LLM-pretraining/OpenWebText/`\n",
        "* For Kaggle: The dataset is loaded by default upon notebook start, from https://www.kaggle.com/datasets/windmaple/openwebtext-gpt2\n",
        "* For GCP: The code checks for the presence of `OpenWebText-gpt2.zip` and assumes that it has been unzipped into the same directory as the notebook"
      ],
      "metadata": {
        "id": "mI1ci-HyMspJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "  data_dir = \"/content/drive/MyDrive/LLM-pretraining/OpenWebText/\" # Assume dataset is already stored there\n",
        "elif platform == \"GCP\":\n",
        "  if not os.path.exists('OpenWebText-gpt2.zip'):\n",
        "    # Assume kaggle binary is in ~/.local/bin after pip install kaggle\n",
        "    !~/.local/bin/kaggle datasets download -d windmaple/OpenWebText-gpt2 && unzip OpenWebText-gpt2.zip\n",
        "  data_dir = \".\"\n",
        "elif platform == \"Kaggle\":  # On Kaggle one should manually add the datasets before running\n",
        "  data_dir = \"/kaggle/input/openwebtext-gpt2\"\n",
        "\n",
        "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "\n",
        "# From: https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/train.py#L116\n",
        "def get_batch(train_or_eval = \"train\"):\n",
        "\n",
        "    data = train_data if train_or_eval == \"train\" else val_data\n",
        "\n",
        "    ix = np.random.randint(0, len(data) - seqlen, (batch_size,))\n",
        "    x = np.stack([(data[i:i+seqlen]).astype(np.int64) for i in ix])\n",
        "    y = np.stack([(data[i+1:i+1+seqlen]).astype(np.int64) for i in ix])\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "rGUFsn1GMuzh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:28.630426Z",
          "iopub.execute_input": "2025-02-21T04:32:28.630623Z",
          "iopub.status.idle": "2025-02-21T04:32:28.647808Z",
          "shell.execute_reply.started": "2025-02-21T04:32:28.630602Z",
          "shell.execute_reply": "2025-02-21T04:32:28.646734Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11b0bcf-153a-44be-bebb-8df4f7fc86c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Define loss function and training step function."
      ],
      "metadata": {
        "id": "BKVSD8KSM1um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@nnx.jit\n",
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: nnx.Module, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ],
      "metadata": {
        "id": "8rRuTmABNV4b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:28.648690Z",
          "iopub.execute_input": "2025-02-21T04:32:28.648880Z",
          "iopub.status.idle": "2025-02-21T04:32:28.683524Z",
          "shell.execute_reply.started": "2025-02-21T04:32:28.648860Z",
          "shell.execute_reply": "2025-02-21T04:32:28.682396Z"
        }
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model and check the model parameter count."
      ],
      "metadata": {
        "id": "ZRRpiYBpwr2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "\n",
        "p_sizes = jax.tree.map(lambda p: p.size if isinstance(p, jnp.ndarray) else 0, nnx.state(model))\n",
        "import operator\n",
        "print(f\"Number of model parameters: {jax.tree.reduce(operator.add, p_sizes)}\")"
      ],
      "metadata": {
        "id": "D_L_PuTkwqtu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:28.684258Z",
          "iopub.execute_input": "2025-02-21T04:32:28.684473Z",
          "iopub.status.idle": "2025-02-21T04:32:33.129248Z",
          "shell.execute_reply.started": "2025-02-21T04:32:28.684451Z",
          "shell.execute_reply": "2025-02-21T04:32:33.128221Z"
        },
        "outputId": "1040c6dc-532c-423f-8716-a6641ce12e85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 124439808\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally check the parameter count against the same model on Hugging Face."
      ],
      "metadata": {
        "id": "nBBJcz1B6XFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# from transformers import GPT2Model\n",
        "\n",
        "# # From https://github.com/huggingface/transformers/issues/27615\n",
        "# model_hf = GPT2Model.from_pretrained('gpt2')\n",
        "# model_parameters = filter(lambda p: p.requires_grad, model_hf.parameters())\n",
        "# param_count = sum([np.prod(p.size()) for p in model_parameters])\n",
        "\n",
        "# print(f\"Number of model parameters from Hugging Face: {param_count}\")"
      ],
      "metadata": {
        "id": "KDWyHtrU6S5f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:33.129850Z",
          "iopub.execute_input": "2025-02-21T04:32:33.130056Z",
          "iopub.status.idle": "2025-02-21T04:32:33.134982Z",
          "shell.execute_reply.started": "2025-02-21T04:32:33.130034Z",
          "shell.execute_reply": "2025-02-21T04:32:33.133692Z"
        }
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "5um2vkeUNckm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schedule = optax.cosine_decay_schedule(\n",
        "  init_value=init_learning_rate,\n",
        "  decay_steps=max_steps\n",
        ")\n",
        "optax_chain = optax.chain(\n",
        "  optax.adamw(learning_rate=schedule, weight_decay=weight_decay)\n",
        ")\n",
        "optimizer = nnx.Optimizer(model, optax_chain)\n",
        "\n",
        "train_metrics = nnx.metrics.Average('loss')\n",
        "val_metrics = nnx.metrics.Average('val_loss')\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"Once upon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
        "print(f\"Initial generated text:\")\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "\n",
        "\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "  'val_loss': []\n",
        "}\n",
        "\n",
        "step = 0\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    input_batch, target_batch = get_batch(\"train\")\n",
        "    if len(input_batch) % len(jax.devices()) != 0: continue  # skip the remaining elements\n",
        "    train_step(model, optimizer, train_metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
        "\n",
        "    if step % 200 == 0:\n",
        "      train_loss = float(train_metrics.compute())\n",
        "      metrics_history['train_loss'].append(train_loss)\n",
        "\n",
        "      elapsed_time = time.time() - start_time\n",
        "      print(f\"Step {step + 1}, Training loss: {train_loss}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "      # eval step\n",
        "      input_val_batch, target_val_batch = get_batch('val')\n",
        "      loss, logits = loss_fn(model, jax.device_put((input_val_batch, target_val_batch), NamedSharding(mesh, P('batch', None))))\n",
        "      val_metrics.update(val_loss=loss, logits=logits)\n",
        "      val_loss = float(val_metrics.compute())\n",
        "      metrics_history['val_loss'].append(val_loss)\n",
        "      wandb.log(data={'val_loss': val_loss, 'train_loss': train_loss}, step=step)\n",
        "      print(f\"Step {step + 1}, Validation loss: {val_loss}\")\n",
        "      train_metrics.reset()\n",
        "      val_metrics.reset()\n",
        "\n",
        "      start_time = time.time()\n",
        "    step += 1\n",
        "\n",
        "    if step > max_steps:\n",
        "      break\n",
        "\n",
        "# Final text generation\n",
        "print(f\"Final generated text:\")\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")"
      ],
      "metadata": {
        "id": "Ysl6CsfENeJN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T04:32:33.135918Z",
          "iopub.execute_input": "2025-02-21T04:32:33.136140Z",
          "iopub.status.idle": "2025-02-21T10:43:50.028572Z",
          "shell.execute_reply.started": "2025-02-21T04:32:33.136118Z",
          "shell.execute_reply": "2025-02-21T10:43:50.027504Z"
        },
        "outputId": "52daa44f-9659-4ef8-9dc3-b926cc2a2059",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial generated text:\n",
            "Once upon a time secured PNG flattened principalsukuukuuku secureduku secured Gothic monopol secureduku secured Gothic Gothic Gothic defenses Gothicuku Gothic Gothic Gothic Gothicuku Caucas organic responseheng Caucas Mel monopolurreduku monopolusions Caucas organicusionsurredurred response response organic monopolurred responseurred monopolusionsusionsurreduku monopolurred monopol Caucas monopol monopolurred response response Xander Caucas monopolurred Caucas principals Xander Caucasurred Caucas grizzurred possess Morales Caucas grizzurred Caucasurred grizz possessurred grizz Caucas grizz Caucas possessurred guaranteeurred guarantee Caucas Lutheranurred Caucas Lutheran possessurred possessStep 1, Training loss: 11.4375, Elapsed Time: 33.53 seconds\n",
            "Step 1, Validation loss: 9.9375\n",
            "Step 201, Training loss: 7.242968559265137, Elapsed Time: 217.61 seconds\n",
            "Step 201, Validation loss: 6.4375\n",
            "Step 401, Training loss: 6.416562557220459, Elapsed Time: 50.98 seconds\n",
            "Step 401, Validation loss: 6.21875\n",
            "Step 601, Training loss: 6.204062461853027, Elapsed Time: 48.24 seconds\n",
            "Step 601, Validation loss: 6.125\n",
            "Step 801, Training loss: 6.033281326293945, Elapsed Time: 42.25 seconds\n",
            "Step 801, Validation loss: 5.78125\n",
            "Step 1001, Training loss: 5.877500057220459, Elapsed Time: 46.13 seconds\n",
            "Step 1001, Validation loss: 5.875\n",
            "Step 1201, Training loss: 5.714218616485596, Elapsed Time: 44.48 seconds\n",
            "Step 1201, Validation loss: 5.6875\n",
            "Step 1401, Training loss: 5.569062232971191, Elapsed Time: 42.26 seconds\n",
            "Step 1401, Validation loss: 5.46875\n",
            "Step 1601, Training loss: 5.411093711853027, Elapsed Time: 42.77 seconds\n",
            "Step 1601, Validation loss: 5.53125\n",
            "Step 1801, Training loss: 5.255468845367432, Elapsed Time: 41.55 seconds\n",
            "Step 1801, Validation loss: 5.0625\n",
            "Step 2001, Training loss: 5.150000095367432, Elapsed Time: 40.41 seconds\n",
            "Step 2001, Validation loss: 5.0\n",
            "Step 2201, Training loss: 5.029531002044678, Elapsed Time: 40.46 seconds\n",
            "Step 2201, Validation loss: 4.9375\n",
            "Step 2401, Training loss: 4.940624713897705, Elapsed Time: 40.48 seconds\n",
            "Step 2401, Validation loss: 4.78125\n",
            "Step 2601, Training loss: 4.82421875, Elapsed Time: 41.57 seconds\n",
            "Step 2601, Validation loss: 4.6875\n",
            "Step 2801, Training loss: 4.725625038146973, Elapsed Time: 40.19 seconds\n",
            "Step 2801, Validation loss: 4.75\n",
            "Step 3001, Training loss: 4.642031192779541, Elapsed Time: 40.39 seconds\n",
            "Step 3001, Validation loss: 4.6875\n",
            "Step 3201, Training loss: 4.547656059265137, Elapsed Time: 40.13 seconds\n",
            "Step 3201, Validation loss: 4.46875\n",
            "Step 3401, Training loss: 4.507343769073486, Elapsed Time: 39.01 seconds\n",
            "Step 3401, Validation loss: 4.46875\n",
            "Step 3601, Training loss: 4.452343463897705, Elapsed Time: 39.84 seconds\n",
            "Step 3601, Validation loss: 4.53125\n",
            "Step 3801, Training loss: 4.392812252044678, Elapsed Time: 39.08 seconds\n",
            "Step 3801, Validation loss: 4.34375\n",
            "Step 4001, Training loss: 4.363749980926514, Elapsed Time: 40.10 seconds\n",
            "Step 4001, Validation loss: 4.21875\n",
            "Step 4201, Training loss: 4.326562404632568, Elapsed Time: 40.11 seconds\n",
            "Step 4201, Validation loss: 4.09375\n",
            "Step 4401, Training loss: 4.282187461853027, Elapsed Time: 39.69 seconds\n",
            "Step 4401, Validation loss: 4.21875\n",
            "Step 4601, Training loss: 4.263281345367432, Elapsed Time: 38.94 seconds\n",
            "Step 4601, Validation loss: 4.15625\n",
            "Step 4801, Training loss: 4.217343807220459, Elapsed Time: 40.57 seconds\n",
            "Step 4801, Validation loss: 4.15625\n",
            "Step 5001, Training loss: 4.213124752044678, Elapsed Time: 40.00 seconds\n",
            "Step 5001, Validation loss: 4.15625\n",
            "Step 5201, Training loss: 4.188827991485596, Elapsed Time: 38.94 seconds\n",
            "Step 5201, Validation loss: 4.25\n",
            "Step 5401, Training loss: 4.161796569824219, Elapsed Time: 39.61 seconds\n",
            "Step 5401, Validation loss: 4.15625\n",
            "Step 5601, Training loss: 4.126953125, Elapsed Time: 40.02 seconds\n",
            "Step 5601, Validation loss: 4.15625\n",
            "Step 5801, Training loss: 4.114062309265137, Elapsed Time: 39.14 seconds\n",
            "Step 5801, Validation loss: 4.1875\n",
            "Step 6001, Training loss: 4.092187404632568, Elapsed Time: 39.11 seconds\n",
            "Step 6001, Validation loss: 4.125\n",
            "Step 6201, Training loss: 4.076640605926514, Elapsed Time: 38.99 seconds\n",
            "Step 6201, Validation loss: 4.125\n",
            "Step 6401, Training loss: 4.059687614440918, Elapsed Time: 39.75 seconds\n",
            "Step 6401, Validation loss: 4.0\n",
            "Step 6601, Training loss: 4.039843559265137, Elapsed Time: 39.07 seconds\n",
            "Step 6601, Validation loss: 3.96875\n",
            "Step 6801, Training loss: 4.028671741485596, Elapsed Time: 39.36 seconds\n",
            "Step 6801, Validation loss: 4.03125\n",
            "Step 7001, Training loss: 4.007890701293945, Elapsed Time: 39.12 seconds\n",
            "Step 7001, Validation loss: 4.09375\n",
            "Step 7201, Training loss: 3.9992969036102295, Elapsed Time: 39.51 seconds\n",
            "Step 7201, Validation loss: 4.0625\n",
            "Step 7401, Training loss: 3.9822654724121094, Elapsed Time: 39.64 seconds\n",
            "Step 7401, Validation loss: 3.84375\n",
            "Step 7601, Training loss: 3.9696874618530273, Elapsed Time: 39.53 seconds\n",
            "Step 7601, Validation loss: 3.984375\n",
            "Step 7801, Training loss: 3.9549217224121094, Elapsed Time: 39.69 seconds\n",
            "Step 7801, Validation loss: 3.984375\n",
            "Step 8001, Training loss: 3.9465625286102295, Elapsed Time: 39.18 seconds\n",
            "Step 8001, Validation loss: 3.8125\n",
            "Step 8201, Training loss: 3.921875, Elapsed Time: 39.60 seconds\n",
            "Step 8201, Validation loss: 3.875\n",
            "Step 8401, Training loss: 3.9287497997283936, Elapsed Time: 39.22 seconds\n",
            "Step 8401, Validation loss: 3.90625\n",
            "Step 8601, Training loss: 3.9120311737060547, Elapsed Time: 39.11 seconds\n",
            "Step 8601, Validation loss: 3.859375\n",
            "Step 8801, Training loss: 3.903749942779541, Elapsed Time: 39.18 seconds\n",
            "Step 8801, Validation loss: 4.03125\n",
            "Step 9001, Training loss: 3.902109384536743, Elapsed Time: 39.38 seconds\n",
            "Step 9001, Validation loss: 3.890625\n",
            "Step 9201, Training loss: 3.9103124141693115, Elapsed Time: 39.15 seconds\n",
            "Step 9201, Validation loss: 3.890625\n",
            "Step 9401, Training loss: 3.8860156536102295, Elapsed Time: 39.54 seconds\n",
            "Step 9401, Validation loss: 3.828125\n",
            "Step 9601, Training loss: 3.8650779724121094, Elapsed Time: 39.77 seconds\n",
            "Step 9601, Validation loss: 4.0625\n",
            "Step 9801, Training loss: 3.859609365463257, Elapsed Time: 39.17 seconds\n",
            "Step 9801, Validation loss: 3.8125\n",
            "Step 10001, Training loss: 3.8500781059265137, Elapsed Time: 39.13 seconds\n",
            "Step 10001, Validation loss: 3.828125\n",
            "Step 10201, Training loss: 3.8515625, Elapsed Time: 39.01 seconds\n",
            "Step 10201, Validation loss: 3.96875\n",
            "Step 10401, Training loss: 3.904531240463257, Elapsed Time: 39.42 seconds\n",
            "Step 10401, Validation loss: 3.921875\n",
            "Step 10601, Training loss: 3.859452962875366, Elapsed Time: 39.43 seconds\n",
            "Step 10601, Validation loss: 3.890625\n",
            "Step 10801, Training loss: 3.833750009536743, Elapsed Time: 39.39 seconds\n",
            "Step 10801, Validation loss: 3.859375\n",
            "Step 11001, Training loss: 3.819999933242798, Elapsed Time: 39.37 seconds\n",
            "Step 11001, Validation loss: 3.828125\n",
            "Step 11201, Training loss: 3.814531087875366, Elapsed Time: 39.05 seconds\n",
            "Step 11201, Validation loss: 3.984375\n",
            "Step 11401, Training loss: 3.8193747997283936, Elapsed Time: 39.02 seconds\n",
            "Step 11401, Validation loss: 3.9375\n",
            "Step 11601, Training loss: 3.8085155487060547, Elapsed Time: 39.02 seconds\n",
            "Step 11601, Validation loss: 3.8125\n",
            "Step 11801, Training loss: 3.7945311069488525, Elapsed Time: 38.99 seconds\n",
            "Step 11801, Validation loss: 3.796875\n",
            "Step 12001, Training loss: 3.786249876022339, Elapsed Time: 39.15 seconds\n",
            "Step 12001, Validation loss: 3.84375\n",
            "Step 12201, Training loss: 3.7864062786102295, Elapsed Time: 39.11 seconds\n",
            "Step 12201, Validation loss: 3.796875\n",
            "Step 12401, Training loss: 3.776484251022339, Elapsed Time: 39.14 seconds\n",
            "Step 12401, Validation loss: 3.609375\n",
            "Step 12601, Training loss: 3.773124933242798, Elapsed Time: 39.17 seconds\n",
            "Step 12601, Validation loss: 3.859375\n",
            "Step 12801, Training loss: 3.762343645095825, Elapsed Time: 39.11 seconds\n",
            "Step 12801, Validation loss: 3.59375\n",
            "Step 13001, Training loss: 3.7509374618530273, Elapsed Time: 39.08 seconds\n",
            "Step 13001, Validation loss: 3.8125\n",
            "Step 13201, Training loss: 3.7512500286102295, Elapsed Time: 39.07 seconds\n",
            "Step 13201, Validation loss: 3.8125\n",
            "Step 13401, Training loss: 3.747734308242798, Elapsed Time: 39.14 seconds\n",
            "Step 13401, Validation loss: 3.796875\n",
            "Step 13601, Training loss: 3.741640567779541, Elapsed Time: 38.94 seconds\n",
            "Step 13601, Validation loss: 3.828125\n",
            "Step 13801, Training loss: 3.736406087875366, Elapsed Time: 38.95 seconds\n",
            "Step 13801, Validation loss: 3.703125\n",
            "Step 14001, Training loss: 3.728281259536743, Elapsed Time: 39.18 seconds\n",
            "Step 14001, Validation loss: 3.953125\n",
            "Step 14201, Training loss: 3.7401561737060547, Elapsed Time: 39.10 seconds\n",
            "Step 14201, Validation loss: 3.84375\n",
            "Step 14401, Training loss: 3.7274999618530273, Elapsed Time: 39.44 seconds\n",
            "Step 14401, Validation loss: 3.65625\n",
            "Step 14601, Training loss: 3.7219531536102295, Elapsed Time: 38.94 seconds\n",
            "Step 14601, Validation loss: 3.6875\n",
            "Step 14801, Training loss: 3.7350780963897705, Elapsed Time: 39.12 seconds\n",
            "Step 14801, Validation loss: 3.828125\n",
            "Step 15001, Training loss: 3.722421884536743, Elapsed Time: 39.18 seconds\n",
            "Step 15001, Validation loss: 3.8125\n",
            "Step 15201, Training loss: 3.723281145095825, Elapsed Time: 39.08 seconds\n",
            "Step 15201, Validation loss: 3.78125\n",
            "Step 15401, Training loss: 3.715078115463257, Elapsed Time: 38.95 seconds\n",
            "Step 15401, Validation loss: 3.765625\n",
            "Step 15601, Training loss: 3.711171865463257, Elapsed Time: 39.17 seconds\n",
            "Step 15601, Validation loss: 3.734375\n",
            "Step 15801, Training loss: 3.695078134536743, Elapsed Time: 39.12 seconds\n",
            "Step 15801, Validation loss: 3.703125\n",
            "Step 16001, Training loss: 3.6957812309265137, Elapsed Time: 38.94 seconds\n",
            "Step 16001, Validation loss: 3.6875\n",
            "Step 16201, Training loss: 3.691171884536743, Elapsed Time: 39.18 seconds\n",
            "Step 16201, Validation loss: 3.8125\n",
            "Step 16401, Training loss: 3.6902341842651367, Elapsed Time: 39.16 seconds\n",
            "Step 16401, Validation loss: 3.671875\n",
            "Step 16601, Training loss: 3.685546875, Elapsed Time: 39.42 seconds\n",
            "Step 16601, Validation loss: 3.71875\n",
            "Step 16801, Training loss: 3.684765577316284, Elapsed Time: 39.05 seconds\n",
            "Step 16801, Validation loss: 3.625\n",
            "Step 17001, Training loss: 3.684765577316284, Elapsed Time: 39.12 seconds\n",
            "Step 17001, Validation loss: 3.609375\n",
            "Step 17201, Training loss: 3.6812498569488525, Elapsed Time: 39.09 seconds\n",
            "Step 17201, Validation loss: 3.703125\n",
            "Step 17401, Training loss: 3.684999942779541, Elapsed Time: 39.00 seconds\n",
            "Step 17401, Validation loss: 3.625\n",
            "Step 17601, Training loss: 3.669999837875366, Elapsed Time: 39.23 seconds\n",
            "Step 17601, Validation loss: 3.78125\n",
            "Step 17801, Training loss: 3.678593635559082, Elapsed Time: 39.00 seconds\n",
            "Step 17801, Validation loss: 3.5625\n",
            "Step 18001, Training loss: 3.6717185974121094, Elapsed Time: 39.07 seconds\n",
            "Step 18001, Validation loss: 3.625\n",
            "Step 18201, Training loss: 3.6709372997283936, Elapsed Time: 39.12 seconds\n",
            "Step 18201, Validation loss: 3.8125\n",
            "Step 18401, Training loss: 3.6731250286102295, Elapsed Time: 39.12 seconds\n",
            "Step 18401, Validation loss: 3.59375\n",
            "Step 18601, Training loss: 3.6557812690734863, Elapsed Time: 39.08 seconds\n",
            "Step 18601, Validation loss: 3.703125\n",
            "Step 18801, Training loss: 3.6653904914855957, Elapsed Time: 39.08 seconds\n",
            "Step 18801, Validation loss: 3.640625\n",
            "Step 19001, Training loss: 3.6544530391693115, Elapsed Time: 39.06 seconds\n",
            "Step 19001, Validation loss: 3.640625\n",
            "Step 19201, Training loss: 3.655156135559082, Elapsed Time: 39.08 seconds\n",
            "Step 19201, Validation loss: 3.75\n",
            "Step 19401, Training loss: 3.6443748474121094, Elapsed Time: 39.09 seconds\n",
            "Step 19401, Validation loss: 3.625\n",
            "Step 19601, Training loss: 3.6449217796325684, Elapsed Time: 39.11 seconds\n",
            "Step 19601, Validation loss: 3.515625\n",
            "Step 19801, Training loss: 3.64453125, Elapsed Time: 39.21 seconds\n",
            "Step 19801, Validation loss: 3.625\n",
            "Step 20001, Training loss: 3.6435155868530273, Elapsed Time: 39.06 seconds\n",
            "Step 20001, Validation loss: 3.703125\n",
            "Step 20201, Training loss: 3.6433591842651367, Elapsed Time: 39.19 seconds\n",
            "Step 20201, Validation loss: 3.53125\n",
            "Step 20401, Training loss: 3.6310155391693115, Elapsed Time: 39.15 seconds\n",
            "Step 20401, Validation loss: 3.546875\n",
            "Step 20601, Training loss: 3.63921856880188, Elapsed Time: 39.07 seconds\n",
            "Step 20601, Validation loss: 3.625\n",
            "Step 20801, Training loss: 3.6232030391693115, Elapsed Time: 39.13 seconds\n",
            "Step 20801, Validation loss: 3.625\n",
            "Step 21001, Training loss: 3.63140606880188, Elapsed Time: 40.12 seconds\n",
            "Step 21001, Validation loss: 3.59375\n",
            "Step 21201, Training loss: 3.625077962875366, Elapsed Time: 39.31 seconds\n",
            "Step 21201, Validation loss: 3.625\n",
            "Step 21401, Training loss: 3.6271092891693115, Elapsed Time: 39.14 seconds\n",
            "Step 21401, Validation loss: 3.71875\n",
            "Step 21601, Training loss: 3.613593578338623, Elapsed Time: 39.17 seconds\n",
            "Step 21601, Validation loss: 3.5\n",
            "Step 21801, Training loss: 3.6210155487060547, Elapsed Time: 39.40 seconds\n",
            "Step 21801, Validation loss: 3.640625\n",
            "Step 22001, Training loss: 3.6156249046325684, Elapsed Time: 39.15 seconds\n",
            "Step 22001, Validation loss: 3.53125\n",
            "Step 22201, Training loss: 3.618828058242798, Elapsed Time: 39.13 seconds\n",
            "Step 22201, Validation loss: 3.71875\n",
            "Step 22401, Training loss: 3.6103124618530273, Elapsed Time: 38.99 seconds\n",
            "Step 22401, Validation loss: 3.625\n",
            "Step 22601, Training loss: 3.6209373474121094, Elapsed Time: 39.10 seconds\n",
            "Step 22601, Validation loss: 3.703125\n",
            "Step 22801, Training loss: 3.6239843368530273, Elapsed Time: 39.12 seconds\n",
            "Step 22801, Validation loss: 3.546875\n",
            "Step 23001, Training loss: 3.6210155487060547, Elapsed Time: 38.95 seconds\n",
            "Step 23001, Validation loss: 3.671875\n",
            "Step 23201, Training loss: 3.6089842319488525, Elapsed Time: 38.94 seconds\n",
            "Step 23201, Validation loss: 3.6875\n",
            "Step 23401, Training loss: 3.603749990463257, Elapsed Time: 39.02 seconds\n",
            "Step 23401, Validation loss: 3.625\n",
            "Step 23601, Training loss: 3.60992169380188, Elapsed Time: 39.29 seconds\n",
            "Step 23601, Validation loss: 3.640625\n",
            "Step 23801, Training loss: 3.602421760559082, Elapsed Time: 38.94 seconds\n",
            "Step 23801, Validation loss: 3.640625\n",
            "Step 24001, Training loss: 3.600468635559082, Elapsed Time: 39.00 seconds\n",
            "Step 24001, Validation loss: 3.5625\n",
            "Step 24201, Training loss: 3.591874837875366, Elapsed Time: 38.94 seconds\n",
            "Step 24201, Validation loss: 3.625\n",
            "Step 24401, Training loss: 3.596328020095825, Elapsed Time: 38.94 seconds\n",
            "Step 24401, Validation loss: 3.71875\n",
            "Step 24601, Training loss: 3.5874998569488525, Elapsed Time: 38.94 seconds\n",
            "Step 24601, Validation loss: 3.59375\n",
            "Step 24801, Training loss: 3.587656259536743, Elapsed Time: 38.94 seconds\n",
            "Step 24801, Validation loss: 3.625\n",
            "Step 25001, Training loss: 3.592968702316284, Elapsed Time: 38.95 seconds\n",
            "Step 25001, Validation loss: 3.578125\n",
            "Step 25201, Training loss: 3.583750009536743, Elapsed Time: 38.95 seconds\n",
            "Step 25201, Validation loss: 3.53125\n",
            "Step 25401, Training loss: 3.584296703338623, Elapsed Time: 38.94 seconds\n",
            "Step 25401, Validation loss: 3.546875\n",
            "Step 25601, Training loss: 3.57867169380188, Elapsed Time: 38.95 seconds\n",
            "Step 25601, Validation loss: 3.515625\n",
            "Step 25801, Training loss: 3.59375, Elapsed Time: 38.94 seconds\n",
            "Step 25801, Validation loss: 3.703125\n",
            "Step 26001, Training loss: 3.5791406631469727, Elapsed Time: 38.94 seconds\n",
            "Step 26001, Validation loss: 3.59375\n",
            "Step 26201, Training loss: 3.587109327316284, Elapsed Time: 38.95 seconds\n",
            "Step 26201, Validation loss: 3.703125\n",
            "Step 26401, Training loss: 3.577890634536743, Elapsed Time: 38.95 seconds\n",
            "Step 26401, Validation loss: 3.609375\n",
            "Step 26601, Training loss: 3.579765558242798, Elapsed Time: 38.95 seconds\n",
            "Step 26601, Validation loss: 3.578125\n",
            "Step 26801, Training loss: 3.5808591842651367, Elapsed Time: 38.95 seconds\n",
            "Step 26801, Validation loss: 3.5625\n",
            "Step 27001, Training loss: 3.5713281631469727, Elapsed Time: 38.94 seconds\n",
            "Step 27001, Validation loss: 3.578125\n",
            "Step 27201, Training loss: 3.5727343559265137, Elapsed Time: 39.08 seconds\n",
            "Step 27201, Validation loss: 3.59375\n",
            "Step 27401, Training loss: 3.57671856880188, Elapsed Time: 39.04 seconds\n",
            "Step 27401, Validation loss: 3.671875\n",
            "Step 27601, Training loss: 3.565312385559082, Elapsed Time: 38.94 seconds\n",
            "Step 27601, Validation loss: 3.625\n",
            "Step 27801, Training loss: 3.575937509536743, Elapsed Time: 38.96 seconds\n",
            "Step 27801, Validation loss: 3.59375\n",
            "Step 28001, Training loss: 3.5704686641693115, Elapsed Time: 38.94 seconds\n",
            "Step 28001, Validation loss: 3.65625\n",
            "Step 28201, Training loss: 3.567031145095825, Elapsed Time: 38.94 seconds\n",
            "Step 28201, Validation loss: 3.515625\n",
            "Step 28401, Training loss: 3.5701560974121094, Elapsed Time: 38.94 seconds\n",
            "Step 28401, Validation loss: 3.59375\n",
            "Step 28601, Training loss: 3.5567967891693115, Elapsed Time: 38.94 seconds\n",
            "Step 28601, Validation loss: 3.53125\n",
            "Step 28801, Training loss: 3.568593740463257, Elapsed Time: 39.15 seconds\n",
            "Step 28801, Validation loss: 3.46875\n",
            "Step 29001, Training loss: 3.5507030487060547, Elapsed Time: 38.94 seconds\n",
            "Step 29001, Validation loss: 3.578125\n",
            "Step 29201, Training loss: 3.5574216842651367, Elapsed Time: 39.03 seconds\n",
            "Step 29201, Validation loss: 3.6875\n",
            "Step 29401, Training loss: 3.5654687881469727, Elapsed Time: 39.02 seconds\n",
            "Step 29401, Validation loss: 3.625\n",
            "Step 29601, Training loss: 3.546952962875366, Elapsed Time: 39.05 seconds\n",
            "Step 29601, Validation loss: 3.71875\n",
            "Step 29801, Training loss: 3.55718731880188, Elapsed Time: 38.96 seconds\n",
            "Step 29801, Validation loss: 3.53125\n",
            "Step 30001, Training loss: 3.5635156631469727, Elapsed Time: 39.29 seconds\n",
            "Step 30001, Validation loss: 3.625\n",
            "Step 30201, Training loss: 3.5532031059265137, Elapsed Time: 39.03 seconds\n",
            "Step 30201, Validation loss: 3.5625\n",
            "Step 30401, Training loss: 3.547187328338623, Elapsed Time: 38.98 seconds\n",
            "Step 30401, Validation loss: 3.671875\n",
            "Step 30601, Training loss: 3.5498437881469727, Elapsed Time: 38.94 seconds\n",
            "Step 30601, Validation loss: 3.546875\n",
            "Step 30801, Training loss: 3.54937481880188, Elapsed Time: 39.00 seconds\n",
            "Step 30801, Validation loss: 3.484375\n",
            "Step 31001, Training loss: 3.559765577316284, Elapsed Time: 38.98 seconds\n",
            "Step 31001, Validation loss: 3.5625\n",
            "Step 31201, Training loss: 3.536249876022339, Elapsed Time: 39.02 seconds\n",
            "Step 31201, Validation loss: 3.5625\n",
            "Step 31401, Training loss: 3.5503904819488525, Elapsed Time: 38.94 seconds\n",
            "Step 31401, Validation loss: 3.578125\n",
            "Step 31601, Training loss: 3.5512499809265137, Elapsed Time: 38.96 seconds\n",
            "Step 31601, Validation loss: 3.546875\n",
            "Step 31801, Training loss: 3.551015615463257, Elapsed Time: 39.06 seconds\n",
            "Step 31801, Validation loss: 3.625\n",
            "Step 32001, Training loss: 3.536562442779541, Elapsed Time: 38.98 seconds\n",
            "Step 32001, Validation loss: 3.46875\n",
            "Step 32201, Training loss: 3.5380468368530273, Elapsed Time: 38.94 seconds\n",
            "Step 32201, Validation loss: 3.4375\n",
            "Step 32401, Training loss: 3.5546092987060547, Elapsed Time: 38.95 seconds\n",
            "Step 32401, Validation loss: 3.53125\n",
            "Step 32601, Training loss: 3.5414843559265137, Elapsed Time: 38.94 seconds\n",
            "Step 32601, Validation loss: 3.484375\n",
            "Step 32801, Training loss: 3.5355467796325684, Elapsed Time: 38.97 seconds\n",
            "Step 32801, Validation loss: 3.515625\n",
            "Step 33001, Training loss: 3.537187337875366, Elapsed Time: 39.36 seconds\n",
            "Step 33001, Validation loss: 3.578125\n",
            "Step 33201, Training loss: 3.53765606880188, Elapsed Time: 39.07 seconds\n",
            "Step 33201, Validation loss: 3.515625\n",
            "Step 33401, Training loss: 3.533515453338623, Elapsed Time: 38.94 seconds\n",
            "Step 33401, Validation loss: 3.515625\n",
            "Step 33601, Training loss: 3.5381250381469727, Elapsed Time: 38.94 seconds\n",
            "Step 33601, Validation loss: 3.5625\n",
            "Step 33801, Training loss: 3.535390615463257, Elapsed Time: 39.22 seconds\n",
            "Step 33801, Validation loss: 3.4375\n",
            "Step 34001, Training loss: 3.5331249237060547, Elapsed Time: 38.94 seconds\n",
            "Step 34001, Validation loss: 3.484375\n",
            "Step 34201, Training loss: 3.541015625, Elapsed Time: 38.94 seconds\n",
            "Step 34201, Validation loss: 3.515625\n",
            "Step 34401, Training loss: 3.541015625, Elapsed Time: 38.95 seconds\n",
            "Step 34401, Validation loss: 3.5625\n",
            "Step 34601, Training loss: 3.5385937690734863, Elapsed Time: 38.95 seconds\n",
            "Step 34601, Validation loss: 3.640625\n",
            "Step 34801, Training loss: 3.5263280868530273, Elapsed Time: 39.09 seconds\n",
            "Step 34801, Validation loss: 3.578125\n",
            "Step 35001, Training loss: 3.5278124809265137, Elapsed Time: 38.95 seconds\n",
            "Step 35001, Validation loss: 3.484375\n",
            "Step 35201, Training loss: 3.527031183242798, Elapsed Time: 38.94 seconds\n",
            "Step 35201, Validation loss: 3.609375\n",
            "Step 35401, Training loss: 3.521250009536743, Elapsed Time: 38.94 seconds\n",
            "Step 35401, Validation loss: 3.609375\n",
            "Step 35601, Training loss: 3.5203123092651367, Elapsed Time: 38.94 seconds\n",
            "Step 35601, Validation loss: 3.609375\n",
            "Step 35801, Training loss: 3.527031183242798, Elapsed Time: 38.94 seconds\n",
            "Step 35801, Validation loss: 3.515625\n",
            "Step 36001, Training loss: 3.522578001022339, Elapsed Time: 39.21 seconds\n",
            "Step 36001, Validation loss: 3.453125\n",
            "Step 36201, Training loss: 3.5168750286102295, Elapsed Time: 38.94 seconds\n",
            "Step 36201, Validation loss: 3.4375\n",
            "Step 36401, Training loss: 3.5171093940734863, Elapsed Time: 38.94 seconds\n",
            "Step 36401, Validation loss: 3.5625\n",
            "Step 36601, Training loss: 3.5353124141693115, Elapsed Time: 39.03 seconds\n",
            "Step 36601, Validation loss: 3.703125\n",
            "Step 36801, Training loss: 3.513359308242798, Elapsed Time: 39.13 seconds\n",
            "Step 36801, Validation loss: 3.53125\n",
            "Step 37001, Training loss: 3.522109270095825, Elapsed Time: 39.16 seconds\n",
            "Step 37001, Validation loss: 3.453125\n",
            "Step 37201, Training loss: 3.524609327316284, Elapsed Time: 38.94 seconds\n",
            "Step 37201, Validation loss: 3.640625\n",
            "Step 37401, Training loss: 3.513749837875366, Elapsed Time: 38.95 seconds\n",
            "Step 37401, Validation loss: 3.5\n",
            "Step 37601, Training loss: 3.513124942779541, Elapsed Time: 38.94 seconds\n",
            "Step 37601, Validation loss: 3.5625\n",
            "Step 37801, Training loss: 3.513124942779541, Elapsed Time: 38.94 seconds\n",
            "Step 37801, Validation loss: 3.515625\n",
            "Step 38001, Training loss: 3.510077953338623, Elapsed Time: 39.49 seconds\n",
            "Step 38001, Validation loss: 3.453125\n",
            "Step 38201, Training loss: 3.512812376022339, Elapsed Time: 38.95 seconds\n",
            "Step 38201, Validation loss: 3.515625\n",
            "Step 38401, Training loss: 3.5149219036102295, Elapsed Time: 39.07 seconds\n",
            "Step 38401, Validation loss: 3.609375\n",
            "Step 38601, Training loss: 3.512343645095825, Elapsed Time: 38.94 seconds\n",
            "Step 38601, Validation loss: 3.484375\n",
            "Step 38801, Training loss: 3.5146093368530273, Elapsed Time: 38.94 seconds\n",
            "Step 38801, Validation loss: 3.40625\n",
            "Step 39001, Training loss: 3.510859251022339, Elapsed Time: 38.94 seconds\n",
            "Step 39001, Validation loss: 3.53125\n",
            "Step 39201, Training loss: 3.5130467414855957, Elapsed Time: 39.07 seconds\n",
            "Step 39201, Validation loss: 3.46875\n",
            "Step 39401, Training loss: 3.515702962875366, Elapsed Time: 38.96 seconds\n",
            "Step 39401, Validation loss: 3.46875\n",
            "Step 39601, Training loss: 3.510937452316284, Elapsed Time: 39.06 seconds\n",
            "Step 39601, Validation loss: 3.515625\n",
            "Step 39801, Training loss: 3.5135154724121094, Elapsed Time: 38.94 seconds\n",
            "Step 39801, Validation loss: 3.4375\n",
            "Step 40001, Training loss: 3.5027341842651367, Elapsed Time: 38.99 seconds\n",
            "Step 40001, Validation loss: 3.46875\n",
            "Step 40201, Training loss: 3.502812385559082, Elapsed Time: 39.04 seconds\n",
            "Step 40201, Validation loss: 3.609375\n",
            "Step 40401, Training loss: 3.504999876022339, Elapsed Time: 39.22 seconds\n",
            "Step 40401, Validation loss: 3.6875\n",
            "Step 40601, Training loss: 3.505937337875366, Elapsed Time: 39.07 seconds\n",
            "Step 40601, Validation loss: 3.59375\n",
            "Step 40801, Training loss: 3.5068750381469727, Elapsed Time: 39.08 seconds\n",
            "Step 40801, Validation loss: 3.5625\n",
            "Step 41001, Training loss: 3.5052342414855957, Elapsed Time: 38.97 seconds\n",
            "Step 41001, Validation loss: 3.46875\n",
            "Step 41201, Training loss: 3.5023436546325684, Elapsed Time: 39.04 seconds\n",
            "Step 41201, Validation loss: 3.609375\n",
            "Step 41401, Training loss: 3.4996092319488525, Elapsed Time: 39.02 seconds\n",
            "Step 41401, Validation loss: 3.484375\n",
            "Step 41601, Training loss: 3.492265462875366, Elapsed Time: 38.95 seconds\n",
            "Step 41601, Validation loss: 3.5\n",
            "Step 41801, Training loss: 3.4939842224121094, Elapsed Time: 39.00 seconds\n",
            "Step 41801, Validation loss: 3.34375\n",
            "Step 42001, Training loss: 3.4934375286102295, Elapsed Time: 39.16 seconds\n",
            "Step 42001, Validation loss: 3.4375\n",
            "Step 42201, Training loss: 3.5026562213897705, Elapsed Time: 38.97 seconds\n",
            "Step 42201, Validation loss: 3.59375\n",
            "Step 42401, Training loss: 3.487968683242798, Elapsed Time: 39.00 seconds\n",
            "Step 42401, Validation loss: 3.453125\n",
            "Step 42601, Training loss: 3.496328115463257, Elapsed Time: 39.02 seconds\n",
            "Step 42601, Validation loss: 3.40625\n",
            "Step 42801, Training loss: 3.4921092987060547, Elapsed Time: 38.94 seconds\n",
            "Step 42801, Validation loss: 3.4375\n",
            "Step 43001, Training loss: 3.5029687881469727, Elapsed Time: 38.99 seconds\n",
            "Step 43001, Validation loss: 3.515625\n",
            "Step 43201, Training loss: 3.4996092319488525, Elapsed Time: 38.99 seconds\n",
            "Step 43201, Validation loss: 3.5\n",
            "Step 43401, Training loss: 3.4864842891693115, Elapsed Time: 38.97 seconds\n",
            "Step 43401, Validation loss: 3.53125\n",
            "Step 43601, Training loss: 3.497734308242798, Elapsed Time: 39.14 seconds\n",
            "Step 43601, Validation loss: 3.59375\n",
            "Step 43801, Training loss: 3.4786717891693115, Elapsed Time: 38.95 seconds\n",
            "Step 43801, Validation loss: 3.46875\n",
            "Step 44001, Training loss: 3.490312337875366, Elapsed Time: 38.95 seconds\n",
            "Step 44001, Validation loss: 3.40625\n",
            "Step 44201, Training loss: 3.479374885559082, Elapsed Time: 38.94 seconds\n",
            "Step 44201, Validation loss: 3.578125\n",
            "Step 44401, Training loss: 3.4806249141693115, Elapsed Time: 39.02 seconds\n",
            "Step 44401, Validation loss: 3.453125\n",
            "Step 44601, Training loss: 3.486406087875366, Elapsed Time: 39.06 seconds\n",
            "Step 44601, Validation loss: 3.4375\n",
            "Step 44801, Training loss: 3.490859270095825, Elapsed Time: 38.96 seconds\n",
            "Step 44801, Validation loss: 3.578125\n",
            "Step 45001, Training loss: 3.491953134536743, Elapsed Time: 38.94 seconds\n",
            "Step 45001, Validation loss: 3.484375\n",
            "Step 45201, Training loss: 3.4896092414855957, Elapsed Time: 39.07 seconds\n",
            "Step 45201, Validation loss: 3.53125\n",
            "Step 45401, Training loss: 3.483828067779541, Elapsed Time: 39.00 seconds\n",
            "Step 45401, Validation loss: 3.421875\n",
            "Step 45601, Training loss: 3.4778125286102295, Elapsed Time: 38.94 seconds\n",
            "Step 45601, Validation loss: 3.484375\n",
            "Step 45801, Training loss: 3.4809374809265137, Elapsed Time: 38.94 seconds\n",
            "Step 45801, Validation loss: 3.40625\n",
            "Step 46001, Training loss: 3.4750781059265137, Elapsed Time: 38.94 seconds\n",
            "Step 46001, Validation loss: 3.546875\n",
            "Step 46201, Training loss: 3.4753124713897705, Elapsed Time: 38.94 seconds\n",
            "Step 46201, Validation loss: 3.546875\n",
            "Step 46401, Training loss: 3.4800000190734863, Elapsed Time: 38.95 seconds\n",
            "Step 46401, Validation loss: 3.59375\n",
            "Step 46601, Training loss: 3.4833593368530273, Elapsed Time: 38.94 seconds\n",
            "Step 46601, Validation loss: 3.59375\n",
            "Step 46801, Training loss: 3.484140634536743, Elapsed Time: 39.06 seconds\n",
            "Step 46801, Validation loss: 3.453125\n",
            "Step 47001, Training loss: 3.468437433242798, Elapsed Time: 38.98 seconds\n",
            "Step 47001, Validation loss: 3.40625\n",
            "Step 47201, Training loss: 3.480781078338623, Elapsed Time: 38.94 seconds\n",
            "Step 47201, Validation loss: 3.5\n",
            "Step 47401, Training loss: 3.486093759536743, Elapsed Time: 38.94 seconds\n",
            "Step 47401, Validation loss: 3.5\n",
            "Step 47601, Training loss: 3.4756250381469727, Elapsed Time: 38.94 seconds\n",
            "Step 47601, Validation loss: 3.453125\n",
            "Step 47801, Training loss: 3.4803123474121094, Elapsed Time: 38.94 seconds\n",
            "Step 47801, Validation loss: 3.4375\n",
            "Step 48001, Training loss: 3.4839842319488525, Elapsed Time: 38.94 seconds\n",
            "Step 48001, Validation loss: 3.6875\n",
            "Step 48201, Training loss: 3.4716405868530273, Elapsed Time: 38.94 seconds\n",
            "Step 48201, Validation loss: 3.484375\n",
            "Step 48401, Training loss: 3.4678125381469727, Elapsed Time: 39.09 seconds\n",
            "Step 48401, Validation loss: 3.421875\n",
            "Step 48601, Training loss: 3.470937490463257, Elapsed Time: 38.94 seconds\n",
            "Step 48601, Validation loss: 3.53125\n",
            "Step 48801, Training loss: 3.470156192779541, Elapsed Time: 38.96 seconds\n",
            "Step 48801, Validation loss: 3.46875\n",
            "Step 49001, Training loss: 3.480156183242798, Elapsed Time: 38.94 seconds\n",
            "Step 49001, Validation loss: 3.5\n",
            "Step 49201, Training loss: 3.468437433242798, Elapsed Time: 38.94 seconds\n",
            "Step 49201, Validation loss: 3.5\n",
            "Step 49401, Training loss: 3.4671874046325684, Elapsed Time: 38.94 seconds\n",
            "Step 49401, Validation loss: 3.453125\n",
            "Step 49601, Training loss: 3.4742186069488525, Elapsed Time: 38.95 seconds\n",
            "Step 49601, Validation loss: 3.4375\n",
            "Step 49801, Training loss: 3.470937490463257, Elapsed Time: 38.94 seconds\n",
            "Step 49801, Validation loss: 3.578125\n",
            "Step 50001, Training loss: 3.4656248092651367, Elapsed Time: 39.06 seconds\n",
            "Step 50001, Validation loss: 3.5625\n",
            "Step 50201, Training loss: 3.476249933242798, Elapsed Time: 38.94 seconds\n",
            "Step 50201, Validation loss: 3.5625\n",
            "Step 50401, Training loss: 3.472109317779541, Elapsed Time: 38.94 seconds\n",
            "Step 50401, Validation loss: 3.453125\n",
            "Step 50601, Training loss: 3.4597654342651367, Elapsed Time: 38.95 seconds\n",
            "Step 50601, Validation loss: 3.4375\n",
            "Step 50801, Training loss: 3.4739062786102295, Elapsed Time: 38.94 seconds\n",
            "Step 50801, Validation loss: 3.5625\n",
            "Step 51001, Training loss: 3.4560937881469727, Elapsed Time: 38.97 seconds\n",
            "Step 51001, Validation loss: 3.59375\n",
            "Step 51201, Training loss: 3.4618749618530273, Elapsed Time: 39.08 seconds\n",
            "Step 51201, Validation loss: 3.5\n",
            "Step 51401, Training loss: 3.460078001022339, Elapsed Time: 39.10 seconds\n",
            "Step 51401, Validation loss: 3.5\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the training loss."
      ],
      "metadata": {
        "id": "thaLs6TD0lt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(metrics_history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B6Eg1Cz2y_iP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T10:43:50.029672Z",
          "iopub.execute_input": "2025-02-21T10:43:50.030074Z",
          "iopub.status.idle": "2025-02-21T10:43:51.584464Z",
          "shell.execute_reply.started": "2025-02-21T10:43:50.030047Z",
          "shell.execute_reply": "2025-02-21T10:43:51.583724Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sensible sentences at the end of the training.\n",
        "\n"
      ],
      "metadata": {
        "id": "26_HtTkZTczd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some training results on Kaggle TPU:\n",
        "\n",
        "| model | params | train loss | val loss | training time (TPU v3)\n",
        "| ------| ------ | ---------- | -------- | -----------------------\n",
        "| gpt2 | 124M         | 3.05  | 3.09     | 6.5 hr\n",
        "| gpt2-medium | 354M  | 2.83  | 2.86     | 22.5 hr\n",
        "\n",
        "The losses are roughly in line with [nanoGPT's](https://github.com/karpathy/nanoGPT?tab=readme-ov-file#baselines)."
      ],
      "metadata": {
        "id": "9hTE4MsEUh9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model saving"
      ],
      "metadata": {
        "id": "m13hUf7CcrSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import orbax.checkpoint as orbax\n",
        "import shutil\n",
        "\n",
        "if platform == \"Colab\":\n",
        "  checkpoint_path = \"/content/checkpoints\"\n",
        "elif platform == \"Kaggle\":\n",
        "  checkpoint_path = \"/kaggle/working/checkpoints\"\n",
        "else:\n",
        "  from pathlib import Path\n",
        "  home = Path.home()\n",
        "  checkpoint_path = os.path.join(str(home), \"checkpoints\")\n",
        "\n",
        "# make sure the folder is empty and usable\n",
        "shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "train_state = nnx.state(model)\n",
        "checkpointer.save(checkpoint_path, train_state)"
      ],
      "metadata": {
        "id": "ofH_VkqMctZ5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T10:43:51.585651Z",
          "iopub.execute_input": "2025-02-21T10:43:51.586015Z",
          "iopub.status.idle": "2025-02-21T10:43:57.582123Z",
          "shell.execute_reply.started": "2025-02-21T10:43:51.585991Z",
          "shell.execute_reply": "2025-02-21T10:43:57.581319Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model restoration\n",
        "Restore the model checkpoint. A pretrained checkpoint is also available [here](https://www.kaggle.com/models/windmaple/gpt2/)."
      ],
      "metadata": {
        "id": "soPqiR1JNmjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nnx.eval_shape(lambda: create_model(rngs=nnx.Rngs(0)))\n",
        "state = nnx.state(model)\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "state = checkpointer.restore(checkpoint_path, item=state)\n",
        "nnx.update(model, state)\n",
        "\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "print(f\"Restored model generated text:\\n{generated_text}\")"
      ],
      "metadata": {
        "id": "EkoFGCgSZ1yz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T10:43:57.583415Z",
          "iopub.execute_input": "2025-02-21T10:43:57.583673Z",
          "iopub.status.idle": "2025-02-21T10:44:12.704383Z",
          "shell.execute_reply.started": "2025-02-21T10:43:57.583648Z",
          "shell.execute_reply": "2025-02-21T10:44:12.703322Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disconnect the Colab runtime"
      ],
      "metadata": {
        "id": "jCApVd7671c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import runtime\n",
        "  runtime.unassign()"
      ],
      "metadata": {
        "id": "NsqYdbrDVKSq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-21T10:44:12.705516Z",
          "iopub.execute_input": "2025-02-21T10:44:12.705749Z",
          "iopub.status.idle": "2025-02-21T10:44:12.710493Z",
          "shell.execute_reply.started": "2025-02-21T10:44:12.705726Z",
          "shell.execute_reply": "2025-02-21T10:44:12.709284Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}